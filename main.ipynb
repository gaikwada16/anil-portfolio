{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/jkAu3SjH6g/puxn9RO39",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaikwada16/anil-portfolio/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6n9yZuWHj90",
        "outputId": "bac7bf2e-740c-4c2b-fa6a-91b68464cfb3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.12-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from textstat) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (4.67.1)\n",
            "Downloading textstat-0.7.12-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.6/176.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.17.2 textstat-0.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "4WBILTbA0F1h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import textstat\n",
        "\n",
        "# =============================\n",
        "# PATHS\n",
        "# =============================\n",
        "INPUT_FILE = \"/content/Input.xlsx\"\n",
        "OUTPUT_STRUCTURE_FILE = \"/content/Output Data Structure.xlsx\"\n",
        "ARTICLES_DIR = \"1extracted_articles\"\n",
        "FINAL_OUTPUT_FILE = \"Final_Output.xlsx\"\n",
        "\n",
        "POSITIVE_WORDS_FILE = \"positive-words.txt\"\n",
        "NEGATIVE_WORDS_FILE = \"negative-words.txt\"\n",
        "\n",
        "os.makedirs(ARTICLES_DIR, exist_ok=True)\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
        "    ),\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# LOAD INPUT\n",
        "# =============================\n",
        "input_df = pd.read_excel(INPUT_FILE)\n",
        "output_structure = pd.read_excel(OUTPUT_STRUCTURE_FILE)"
      ],
      "metadata": {
        "id": "X7kwuswE_K46"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "POSITIVE_WORDS_FILE = \"positive-words.txt\"\n",
        "NEGATIVE_WORDS_FILE = \"negative-words.txt\"\n",
        "\n",
        "def download_if_missing(url, filename):\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "download_if_missing(\n",
        "    \"https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/data/opinion-lexicon-English/positive-words.txt\",\n",
        "    POSITIVE_WORDS_FILE\n",
        ")\n",
        "\n",
        "download_if_missing(\n",
        "    \"https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/data/opinion-lexicon-English/negative-words.txt\",\n",
        "    NEGATIVE_WORDS_FILE\n",
        ")\n"
      ],
      "metadata": {
        "id": "N_hvEgoe_Rdq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# LOAD SENTIMENT LEXICONS\n",
        "# =============================\n",
        "def load_words(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "        return set(\n",
        "            word.strip().lower()\n",
        "            for word in f\n",
        "            if word.strip() and not word.startswith(\";\")\n",
        "        )\n",
        "\n",
        "positive_words = load_words(POSITIVE_WORDS_FILE)\n",
        "negative_words = load_words(NEGATIVE_WORDS_FILE)\n"
      ],
      "metadata": {
        "id": "KfBSsg2iAsDZ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEE4nbY_J1ND",
        "outputId": "ed2e30da-f8a2-4a6e-ae99-d719aa9cc0f3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# LOAD STOPWORDS\n",
        "# =============================\n",
        "stopwords = set()\n",
        "\n",
        "stopword_files = [\n",
        "    \"StopWords_Auditor.txt\",\n",
        "    \"StopWords_Currencies.txt\",\n",
        "    \"StopWords_DatesandNumbers.txt\",\n",
        "    \"StopWords_Generic.txt\",\n",
        "    \"StopWords_GenericLong.txt\",\n",
        "    \"StopWords_Geographic.txt\",\n",
        "    \"StopWords_Names.txt\"\n",
        "]\n",
        "\n",
        "for file in stopword_files:\n",
        "    try:\n",
        "        with open(file, \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "            for line in f:\n",
        "                word = line.strip().lower()\n",
        "                if word and not word.startswith(\";\"):\n",
        "                    stopwords.add(word)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Missing stopword file: {file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1ZcMJqNKitY",
        "outputId": "eb6bd667-f928-4ff7-dd14-b37b24fc2d98"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing stopword file: StopWords_Auditor.txt\n",
            "Missing stopword file: StopWords_Currencies.txt\n",
            "Missing stopword file: StopWords_DatesandNumbers.txt\n",
            "Missing stopword file: StopWords_Generic.txt\n",
            "Missing stopword file: StopWords_GenericLong.txt\n",
            "Missing stopword file: StopWords_Geographic.txt\n",
            "Missing stopword file: StopWords_Names.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# TEXT ANALYSIS\n",
        "# =============================\n",
        "results = []\n",
        "\n",
        "def count_syllables(word):\n",
        "    return textstat.syllable_count(word)\n",
        "\n",
        "for _, row in input_df.iterrows():\n",
        "    url_id = row[\"URL_ID\"]\n",
        "    url = row[\"URL\"]\n",
        "\n",
        "    file_path = os.path.join(ARTICLES_DIR, f\"{url_id}.txt\")\n",
        "    if not os.path.exists(file_path):\n",
        "        continue\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read().lower()\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # ✅ STOPWORD FIX APPLIED HERE\n",
        "    words_clean = [\n",
        "        w for w in words\n",
        "        if w.isalpha() and w not in stopwords\n",
        "    ]\n",
        "\n",
        "    word_count = len(words_clean)\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    # =============================\n",
        "    # SENTIMENT\n",
        "    # =============================\n",
        "    pos_score = sum(1 for w in words_clean if w in positive_words)\n",
        "    neg_score = sum(1 for w in words_clean if w in negative_words)\n",
        "\n",
        "    polarity = (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
        "    subjectivity = (pos_score + neg_score) / (word_count + 0.000001)\n",
        "\n",
        "    # =============================\n",
        "    # READABILITY\n",
        "    # =============================\n",
        "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "\n",
        "    complex_words = [w for w in words_clean if count_syllables(w) > 2]\n",
        "    complex_word_count = len(complex_words)\n",
        "\n",
        "    percentage_complex_words = complex_word_count / word_count if word_count else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    avg_words_per_sentence = avg_sentence_length\n",
        "\n",
        "    syllables_per_word = (\n",
        "        sum(count_syllables(w) for w in words_clean) / word_count\n",
        "        if word_count else 0\n",
        "    )\n",
        "\n",
        "    # =============================\n",
        "    # PERSONAL PRONOUNS\n",
        "    # =============================\n",
        "    pronouns = re.findall(r\"\\b(I|we|my|ours|us)\\b\", text, re.I)\n",
        "    personal_pronouns = len(pronouns)\n",
        "\n",
        "    # =============================\n",
        "    # AVG WORD LENGTH\n",
        "    # =============================\n",
        "    avg_word_length = (\n",
        "        sum(len(w) for w in words_clean) / word_count if word_count else 0\n",
        "    )\n",
        "\n",
        "    results.append([\n",
        "        url_id, url, pos_score, neg_score, polarity, subjectivity,\n",
        "        avg_sentence_length, percentage_complex_words, fog_index,\n",
        "        avg_words_per_sentence, complex_word_count, word_count,\n",
        "        syllables_per_word, personal_pronouns, avg_word_length\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "49-P6NLDB615"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-k1ATCdPQQg",
        "outputId": "5257dbd3-2e34-4731-e77f-f903ab6406c0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "146"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# SAVE FINAL OUTPUT\n",
        "# =============================\n",
        "final_df = pd.DataFrame(results, columns=output_structure.columns)\n",
        "final_df.to_excel(FINAL_OUTPUT_FILE, index=False)\n",
        "\n",
        "print(\"✅ Extraction + Analysis completed\")\n",
        "print(f\"✅ Output saved as {FINAL_OUTPUT_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkIhgXzKCj-J",
        "outputId": "235feba3-38ce-483f-f987-84f99b1e68ce"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extraction + Analysis completed\n",
            "✅ Output saved as Final_Output.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "final_df = pd.read_excel(\"Output Data Structure.xlsx\")\n",
        "structure_df = pd.read_excel(\"/content/Output Data Structure.xlsx\")\n",
        "\n",
        "# 1. Column order check\n",
        "assert list(final_df.columns) == list(structure_df.columns), \"❌ Column order mismatch\"\n",
        "\n",
        "# 2. Row count check\n",
        "assert len(final_df) > 0, \"❌ No rows generated\"\n",
        "\n",
        "# 3. Null check\n",
        "assert final_df.isnull().sum().sum() == 0, \"❌ Null values found\"\n",
        "\n",
        "# 4. URL_ID uniqueness\n",
        "assert final_df[\"URL_ID\"].is_unique, \"❌ Duplicate URL_IDs\"\n",
        "\n",
        "print(\"✅ Validation passed: Final_Output.xlsx is correct\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAE2T9-VJdTn",
        "outputId": "6645d4ee-32ed-4cf7-ba23-3f748ddb29e7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Validation passed: Final_Output.xlsx is correct\n"
          ]
        }
      ]
    }
  ]
}